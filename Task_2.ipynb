{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8fd744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54631557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "797450df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1464\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./VOC2012\"  # root folder containing VOCdevkit/\n",
    "full_dataset = VOCSegmentation(root=data_dir, image_set='train', download=False)\n",
    "print(f\"Total images: {len(full_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac40d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset\n",
    "subset_size = 15\n",
    "random.seed(42)  # for reproducibility\n",
    "subset_indices = random.sample(range(len(full_dataset)), subset_size)\n",
    "subset_dataset = Subset(full_dataset, subset_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0badfb",
   "metadata": {},
   "source": [
    "## Spliting into 80/20% train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3218c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 12, Test size: 3\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * subset_size)\n",
    "test_size = subset_size - train_size\n",
    "\n",
    "train_indices = subset_indices[:train_size]\n",
    "test_indices = subset_indices[train_size:]\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "080cc11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCSubsetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, size=224):\n",
    "        self.subset = subset\n",
    "        self.size = size\n",
    "        # transformations excluding resizing\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, mask = self.subset[idx]  # PIL images\n",
    "        \n",
    "        # Resize PIL images first\n",
    "        img = TF.resize(img, (self.size, self.size))\n",
    "        mask = TF.resize(mask, (self.size, self.size), interpolation=TF.InterpolationMode.NEAREST)\n",
    "        \n",
    "        # Apply tensor conversion and normalization\n",
    "        img = self.img_transform(img)\n",
    "        mask = torch.as_tensor(np.array(mask), dtype=torch.long)\n",
    "        \n",
    "        return img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbab10d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap subsets with your transform\n",
    "train_data = VOCSubsetDataset(train_dataset, size=256)\n",
    "test_data  = VOCSubsetDataset(test_dataset, size=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1b419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5092d14",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca6ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mask (PIL Image) to class indices\n",
    "def mask_to_class(mask):\n",
    "    mask = np.array(mask)\n",
    "    # VOC has 21 classes, background=0\n",
    "    return torch.from_numpy(mask).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd30e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def mask_to_class(mask):\n",
    "    \"\"\"\n",
    "    Converts VOC mask (PIL Image or numpy array) to class indices tensor\n",
    "    \"\"\"\n",
    "    if isinstance(mask, np.ndarray):\n",
    "        mask_np = mask\n",
    "    else:\n",
    "        mask_np = np.array(mask)\n",
    "    # If mask has RGB, map colors to class indices (VOC has 21 classes)\n",
    "    # Here, assume mask already has class indices (0-20)\n",
    "    return torch.as_tensor(mask_np, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e020fc6",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcc24193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class FCN32s(nn.Module):\n",
    "    def __init__(self, num_classes=21, pretrained=True, upsample_method='bilinear'):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16(pretrained=pretrained)\n",
    "        self.features = vgg.features  # convolutional backbone\n",
    "        \n",
    "        # Replace FC layers with conv layers\n",
    "        self.conv6 = nn.Conv2d(512, 4096, kernel_size=7)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=1)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.drop7 = nn.Dropout2d()\n",
    "        self.score = nn.Conv2d(4096, num_classes, kernel_size=1)\n",
    "        \n",
    "        self.upsample_method = upsample_method\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.relu6(self.conv6(x))\n",
    "        x = self.drop6(x)\n",
    "        x = self.relu7(self.conv7(x))\n",
    "        x = self.drop7(x)\n",
    "        x = self.score(x)\n",
    "        \n",
    "        if self.upsample_method == 'bilinear':\n",
    "            x = F.interpolate(x, size=(256,256), mode='bilinear', align_corners=False)\n",
    "        else:\n",
    "            x = nn.ConvTranspose2d(21, 21, kernel_size=64, stride=32, padding=16, bias=False)(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "num_classes = 21  # Pascal VOC\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = FCN32s(num_classes=num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0362ed0d",
   "metadata": {},
   "source": [
    "## Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ef9660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da46f03",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "(a) Pixel Accuracy\n",
    "\n",
    "Fraction of correctly classified pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a89c3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pixel_accuracy(outputs, masks):\n",
    "    \"\"\"\n",
    "    outputs: [B, C, H, W] raw logits\n",
    "    masks:   [B, H, W] ground truth class index (0â€“20)\n",
    "    \"\"\"\n",
    "    preds = outputs.argmax(dim=1)              # [B, H, W]\n",
    "    correct = (preds == masks).float()\n",
    "    acc = correct.sum() / correct.numel()\n",
    "    return acc.item()\n",
    "\n",
    "\n",
    "def mean_iou(pred, mask, num_classes=21):\n",
    "    pred = pred.argmax(1).cpu().numpy()\n",
    "    mask = mask.cpu().numpy()\n",
    "\n",
    "    ious = []\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls)\n",
    "        mask_cls = (mask == cls)\n",
    "\n",
    "        intersection = (pred_cls & mask_cls).sum()\n",
    "        union = (pred_cls | mask_cls).sum()\n",
    "\n",
    "        if union == 0:\n",
    "            continue\n",
    "\n",
    "        iou = intersection / union\n",
    "        ious.append(iou)\n",
    "\n",
    "    return np.mean(ious) if len(ious) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151607d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  Loss: 2.1349\n",
      "  Pixel Acc: 0.4307\n",
      "  mIoU: 0.1304\n",
      "Epoch 2/20\n",
      "  Loss: 1.2064\n",
      "  Pixel Acc: 0.6117\n",
      "  mIoU: 0.2066\n",
      "Epoch 3/20\n",
      "  Loss: 0.9855\n",
      "  Pixel Acc: 0.6434\n",
      "  mIoU: 0.2292\n",
      "Epoch 4/20\n",
      "  Loss: 1.0854\n",
      "  Pixel Acc: 0.6325\n",
      "  mIoU: 0.2429\n",
      "Epoch 5/20\n",
      "  Loss: 0.9568\n",
      "  Pixel Acc: 0.6236\n",
      "  mIoU: 0.2440\n",
      "Epoch 6/20\n",
      "  Loss: 0.9151\n",
      "  Pixel Acc: 0.6306\n",
      "  mIoU: 0.2070\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import functional as TF\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_pixacc = 0\n",
    "    epoch_miou = 0\n",
    "\n",
    "    for imgs, masks in train_loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_pixacc += pixel_accuracy(outputs, masks)\n",
    "        epoch_miou += mean_iou(outputs, masks)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "    print(f\"  Pixel Acc: {epoch_pixacc/len(train_loader):.4f}\")\n",
    "    print(f\"  mIoU: {epoch_miou/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7449e21a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m10\u001b[39m,\u001b[32m4\u001b[39m))\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Loss curve\u001b[39;00m\n\u001b[32m      4\u001b[39m plt.subplot(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Loss curve\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1, num_epochs+1), train_losses, marker='o', label='Train Loss')\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Pixel accuracy curve\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(1, num_epochs+1), train_pixel_acc, marker='o', color='green', label='Train Pixel Acc')\n",
    "plt.title(\"Training Pixel Accuracy Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Pixel Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
